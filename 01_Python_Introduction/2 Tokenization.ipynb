{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcca4333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m943.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.1\n",
      "  Downloading nltk-3.8-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Using cached regex-2022.10.31-cp38-cp38-macosx_10_9_x86_64.whl (294 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8 regex-2022.10.31 textblob-0.17.1 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd48e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb8c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24322151",
   "metadata": {},
   "source": [
    "* In order to do tokenization, we can access tokens by calling words from the TextBlob object. As a result, you will see that the text we have is allocated to tokens as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d67950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Hello', 'everyone', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d916703",
   "metadata": {},
   "source": [
    "* Let’s do this with the NLTK (Natural Language Toolkit) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e13843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3b5268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5b23a",
   "metadata": {},
   "source": [
    "* As you can see, we have called word_tokenize and sent_tokenize objects from the NLTK library. With sent_tokenize we’ll be able to split the text into sentences. We’ll split it into the same text words with word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad32c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone!', 'Welcome to my blog post on Medium.', 'We are studying Natural Language Processing.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sents = nltk.sent_tokenize(text)\n",
    "print(tokens_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e5ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '!', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
