{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124760a9",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "\n",
    "* Why should one pre-process text, anyway? It is because computers are best at understanding numerical data. So, we convert strings into numerical form and then pass this numerical data into models to make them work.\n",
    "\n",
    "\n",
    "* We’ll be looking into techniques like Tokenization, Normalization, Stemming, Lemmatization, Corpus, Stopwords, Part of speech, a bag of words, n-grams, and word embedding. These techniques are enough to make a computer understand data with the text.\n",
    "\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "* It is the process of converting long strings of text into smaller pieces or tokens, hence the name- Tokenization.\n",
    "\n",
    "* Suppose we have a string like, “Tokenize this sentence for the testing purposes.”\n",
    "\n",
    "* In this case, after tokenization is processed the sentence would look like, \n",
    "\n",
    " **{“Tokenize”, “this”, “sentence”, “for”, “the”, “testing”, “purpose”, “.”}**\n",
    "\n",
    "\n",
    "* This would be an example of- word tokenization, we can perform characterized tokenization similarly.\n",
    "\n",
    "\n",
    "\n",
    "### Normalization\n",
    "* It is the process of generalizing all words by converting these into the same case, removing punctuations, expanding contractions, or converting words to their equivalents.\n",
    "\n",
    "* Normalization would get rid of punctuations and case-sensitivity in the aforementioned example and our sentence would then look like this, {“tokenize”, “this”, “sentence”, “for”, “the”, “testing”, “purpose”}.\n",
    "\n",
    "\n",
    "### Stemming\n",
    "* Stemming is the process of removing affixes from a word. \n",
    "* For example, **running** will be converted to **run**. \n",
    "* So after Stemming, our sentence would look like, {“tokenize”, “this”, “sentence”, “for”, “the”, “test”, “purpose”}.\n",
    "\n",
    "\n",
    "\n",
    "### Lemmatization\n",
    "* Lemmatization is one of the most common text pre-processing techniques used in Natural Language Processing (NLP) and machine learning in general.\n",
    "\n",
    "* Both in stemming and in lemmatization, we try to reduce a given word to its root word. The root word is called a stem in the stemming process, and it is called a lemma in the lemmatization process. But there are a few more differences to the two than that.\n",
    "\n",
    "\n",
    "#### How is Lemmatization different from Stemming????\n",
    "\n",
    "* In stemming, a part of the word is just chopped off at the tail end to arrive at the stem of the word. There are definitely different algorithms used to find out how many characters have to be chopped off, but the algorithms don’t actually know the meaning of the word in the language it belongs to. In lemmatization, on the other hand, the algorithms have this knowledge. In fact, you can even say that these algorithms refer a dictionary to understand the meaning of the word before reducing it to its root word, or lemma.\n",
    "\n",
    "* So, a lemmatization algorithm would know that the word better is derived from the word good, and hence, the lemme is good. But a stemming algorithm wouldn’t be able to do the same. There could be over-stemming or under-stemming, and the word better could be reduced to either bet, or bett, or just retained as better. But there is no way in stemming that it could be reduced to its root word good. This, basically is the difference between stemming and lemmatization.\n",
    "\n",
    "\n",
    "\n",
    "#### Advantages and Disadvantages of Lemmatization\n",
    "* As you could probably tell by now, the obvious advantage of lemmatization is that it is more accurate. So if you’re dealing with an NLP application such as a chat bot or a virtual assistant where understanding the meaning of the dialogue is crucial, lemmatization would be useful. But this accuracy comes at a cost.\n",
    "\n",
    "* Because lemmatization involves deriving the meaning of a word from something like a dictionary, it’s very time consuming. So most lemmatization algorithms are slower compared to their stemming counterparts. There is also a computation overhead for lemmatization, however, in an ML problem, computational resources are rarely a cause of concern.\n",
    "\n",
    "\n",
    "### Corpus\n",
    "* Corpus or body in Latin, is the collection of text. It refers to the collection generated from our text data. You might see corpora in some places which is the plural form of the corpus. It is the dictionary for our NLP models. Computers work with numbers instead of strings, so all these strings are represented in numerical forms as follows: {“tokenize”:1, “this”:2, “sentence”:3, “for”:4, “the”:5, “test”:6, “purpose”:7}\n",
    "\n",
    "### Stop words\n",
    "* There are some words in a sentence that play no part in the context or meaning of the sentence. These words are called Stop words. Before passing data as input we remove them from the corpus. Stop words include words like, “the”, “a”, “and”. These words tend to occur frequently in a sentence structure.\n",
    "\n",
    "\n",
    "### Part of Speech Tagging\n",
    "* Part of Speech Tagging (POS-Tag) is the labeling of the words in a text according to their word types (noun, adjective, adverb, verb, etc.). \n",
    "\n",
    "\n",
    "* It is a process of converting a sentence to forms — list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.\n",
    "\n",
    "* Let’s examine the most used tags with examples.\n",
    "\n",
    "* Noun (N)- Daniel, London, table, dog, teacher, pen, city, happiness, hope\n",
    "* Verb (V)- go, speak, run, eat, play, live, walk, have, like, are, is\n",
    "* Adjective(ADJ)- big, happy, green, young, fun, crazy, three\n",
    "* Adverb(ADV)- slowly, quietly, very, always, never, too, well, tomorrow\n",
    "* Preposition (P)- at, on, in, from, with, near, between, about, under\n",
    "* Conjunction (CON)- and, or, but, because, so, yet, unless, since, if\n",
    "* Pronoun(PRO)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    "* Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
